// Auto-generated by web/scripts/generate_types.py
// Do not edit manually. Update tooltips in web/scripts/ui_metadata.py.

/** Tooltip text for config fields, keyed by dot-notation field path. */
export const FIELD_TOOLTIPS: Record<string, string> = {
  "MuonWithAuxAdam": "MuonWithAuxAdam",
  "Simplified_AdEMAMix": "Simplified AdEMAMix",
  "accelerated_ns": "Accelerated ns",
  "adam_debias": "Adam debias",
  "adam_w_mode": "Adam w mode",
  "adanorm": "Adanorm",
  "additional_embedding.initial_embedding_text": "The initial embedding text used when creating a new embedding",
  "additional_embedding.is_output_embedding": "Output embeddings are calculated at the output of the text encoder, not the input. This can improve results for larger text encoders and lower VRAM usage.",
  "additional_embedding.model_name": "The base embedding to train on. Leave empty to create a new embedding",
  "additional_embedding.placeholder": "The placeholder used when using the embedding in a prompt",
  "additional_embedding.stop_training_after": "When to stop training the embedding",
  "additional_embedding.token_count": "The token count used when creating a new embedding. Leave empty to auto detect from the initial embedding text.",
  "additional_embeddings": "Additional embeddings",
  "alpha": "Alpha",
  "alpha_grad": "Alpha grad",
  "ams_bound": "Ams bound",
  "amsgrad": "Amsgrad",
  "api_key": "Api key",
  "approx_mars": "Approx mars",
  "aspect_ratio_bucketing": "Aspect ratio bucketing enables training on images with different aspect ratios",
  "async_gradient_reduce": "Multi-GPU: Asynchronously start the gradient reduce operations during the backward pass. Can be more efficient, but requires some VRAM.",
  "async_gradient_reduce_buffer": "Multi-GPU: Maximum VRAM for \"Async Gradient Reduce\", in megabytes. A multiple of this value can be needed if combined with \"Fused Back Pass\" and/or \"Layer offload fraction\"",
  "attention_mask": "Attention mask",
  "auto_kappa_p": "Auto kappa p",
  "backup_after": "The interval used when automatically creating model backups during training",
  "backup_after_unit": "Backup after unit",
  "backup_before_save": "Create a full backup before saving the final model",
  "balancing": "Balancing",
  "balancing_strategy": "Balancing strategy",
  "base_image_path": "Base image path",
  "base_model_name": "Filename, directory or Hugging Face repository of the base model",
  "batch_size": "The batch size of one training step. If you use multiple GPUs, this is the batch size of each GPU (local batch size).",
  "beta1": "Beta1",
  "beta1_warmup": "Beta1 warmup",
  "beta2": "Beta2",
  "beta2_normuon": "Beta2 normuon",
  "beta3": "Beta3",
  "beta3_ema": "Beta3 ema",
  "bias_correction": "Bias correction",
  "block_wise": "Block wise",
  "bundle_additional_embeddings": "Bundles any additional embeddings into the LoRA output file, rather than as separate files",
  "cache_dir": "The directory where cached data is saved",
  "caps_randomize_enable": "Caps randomize enable",
  "caps_randomize_lowercase": "Caps randomize lowercase",
  "caps_randomize_mode": "Caps randomize mode",
  "caps_randomize_probability": "Caps randomize probability",
  "capturable": "Capturable",
  "cautious": "Cautious",
  "cautious_mask": "Cautious mask",
  "cautious_wd": "Cautious wd",
  "centered": "Centered",
  "cfg_scale": "Cfg scale",
  "clear_cache_before_training": "Clears the cache directory before starting to train. Only disable this if you want to continue using the same cached data. Disabling this can lead to errors, if other settings are changed during a restart",
  "clip_grad_norm": "Clips the gradient norm. Leave empty to disable gradient clipping.",
  "clip_threshold": "Clip threshold",
  "cloud": "Cloud",
  "cloud.create": "Automatically creates a new cloud instance if both Host:Port and Cloud ID are empty. Currently supported for RUNPOD.",
  "cloud.delete_workspace": "Delete the workspace directory on the cloud after training has finished successfully and data has been downloaded.",
  "cloud.detach_trainer": "Allows the trainer to keep running even if your connection to the cloud is lost.",
  "cloud.download_backups": "Download backups from the remote workspace directory to your local machine. It's usually not necessary to download them, because as long as the backups are still available on the cloud, the training can be restarted using one of the cloud's backups.",
  "cloud.download_output_model": "Download the final model after training. You can disable this if you plan to use an automatically saved checkpoint instead.",
  "cloud.download_samples": "Download samples from the remote workspace directory to your local machine.",
  "cloud.download_saves": "Download the automatically saved training checkpoints from the remote workspace directory to your local machine.",
  "cloud.download_tensorboard": "Download TensorBoard event logs from the remote workspace directory to your local machine. They can then be viewed locally in TensorBoard. It is recommended to disable \"Sample to TensorBoard\" to reduce the event log size.",
  "cloud.enabled": "Enable cloud training",
  "cloud.file_sync": "Choose NATIVE_SCP to use scp.exe to transfer files. FABRIC_SFTP uses the Paramiko/Fabric SFTP implementation for file transfers instead.",
  "cloud.gpu_type": "Select the GPU type. Enter an API key before pressing the button.",
  "cloud.huggingface_cache_dir": "Huggingface models are downloaded to this remote directory.",
  "cloud.install_cmd": "The command for installing OneTrainer. Leave the default, unless you want to use a development branch of OneTrainer.",
  "cloud.install_onetrainer": "Automatically install OneTrainer from GitHub if the directory doesn't already exist.",
  "cloud.min_download": "Set the minimum download speed of the cloud in Mbps.",
  "cloud.name": "The name of the new cloud instance.",
  "cloud.on_detached_error": "What to do if training stops due to an error, but the client has been detached and cannot download data. Data may be lost.",
  "cloud.on_detached_finish": "What to do when training finishes, but the client has been detached and cannot download data. Data may be lost.",
  "cloud.on_error": "What to do if training stops due to an error: Stop or delete the cloud, or do nothing. Data may be lost.",
  "cloud.on_finish": "What to do when training finishes and the data has been fully downloaded: Stop or delete the cloud, or do nothing.",
  "cloud.onetrainer_dir": "The directory for OneTrainer on the cloud.",
  "cloud.remote_dir": "The directory on the cloud where files will be uploaded and downloaded.",
  "cloud.run_id": "An id identifying the remotely running trainer. In case you have lost connection or closed OneTrainer, it will try to reattach to this id instead of starting a new remote trainer.",
  "cloud.sub_type": "Select the RunPod cloud type. See RunPod's website for details.",
  "cloud.tensorboard_tunnel": "Instead of starting tensorboard locally, make a TCP tunnel to a tensorboard on the cloud",
  "cloud.type": "Choose LINUX to connect to a linux machine via SSH. Choose RUNPOD for additional functionality such as automatically creating and deleting pods.",
  "cloud.update_onetrainer": "Update OneTrainer if it already exists on the cloud.",
  "cloud.volume_size": "Set the storage volume size in GB. This volume persists only until the cloud is deleted - not a RunPod network volume",
  "coft_eps": "The control strength of COFT. Only has an effect if COFT is enabled.",
  "compile": "Uses torch.compile and Triton to significantly speed up training. Only applies to transformer/unet. Disable in case of compatibility issues.",
  "concept.balancing": "The number of samples used during training. Use repeats to multiply the concept, or samples to specify an exact number of samples used in each epoch.",
  "concept.enabled": "Enable or disable this concept",
  "concept.image.enable_crop_jitter": "Enables random cropping of samples",
  "concept.image.enable_fixed_flip": "Apply a fixed horizontal flip to all images in this concept",
  "concept.image.enable_fixed_rotate": "Apply a fixed rotation to all images in this concept",
  "concept.image.enable_random_brightness": "Randomly adjusts the brightness of the sample during training",
  "concept.image.enable_random_circular_mask_shrink": "Automatically create circular masks for masked training",
  "concept.image.enable_random_contrast": "Randomly adjusts the contrast of the sample during training",
  "concept.image.enable_random_flip": "Randomly flip the sample during training",
  "concept.image.enable_random_hue": "Randomly adjusts the hue of the sample during training",
  "concept.image.enable_random_mask_rotate_crop": "Randomly rotate the training samples and crop to the masked region",
  "concept.image.enable_random_rotate": "Randomly rotates the sample during training",
  "concept.image.enable_random_saturation": "Randomly adjusts the saturation of the sample during training",
  "concept.image.enable_resolution_override": "Override the resolution for this concept. Optionally specify multiple resolutions separated by a comma, or a single exact resolution in the format <width>x<height>",
  "concept.image_variations": "The number of different image versions to cache if latent caching is enabled.",
  "concept.include_subdirectories": "Includes images from subdirectories into the dataset",
  "concept.loss_weight": "The loss multiplier for this concept.",
  "concept.name": "Name of the concept",
  "concept.path": "Path where the training data is located",
  "concept.text.caps_randomize_enable": "Enables randomization of capitalization for tags in the caption.",
  "concept.text.caps_randomize_lowercase": "If enabled, converts the caption to lowercase before any further processing.",
  "concept.text.caps_randomize_mode": "Comma-separated list of types of capitalization randomization to perform. 'capslock' for ALL CAPS, 'title' for First Letter Of Every Word, 'first' for First word only, 'random' for randomized letters.",
  "concept.text.caps_randomize_probability": "Probability to randomize capitalization of each tag, from 0 to 1.",
  "concept.text.enable_tag_shuffling": "Enables tag shuffling",
  "concept.text.keep_tags_count": "The number of tags at the start of the caption that are not shuffled or dropped",
  "concept.text.prompt_source": "The source for prompts used during training. When selecting 'From single text file', select a text file that contains a list of prompts",
  "concept.text.tag_delimiter": "The delimiter between tags",
  "concept.text.tag_dropout_enable": "Enables random dropout for tags in the captions.",
  "concept.text.tag_dropout_mode": "Method used to drop captions. 'Full' will drop the entire caption past the 'kept' tags with a certain probability, 'Random' will drop individual tags with the set probability, and 'Random Weighted' will linearly increase the probability of dropping tags, more likely to preserve tags near the front with full probability to drop at the end.",
  "concept.text.tag_dropout_probability": "Probability to drop tags, from 0 to 1.",
  "concept.text.tag_dropout_special_tags": "List of tags which will be whitelisted/blacklisted by dropout. 'Whitelist' tags will never be dropped but all others may be, 'Blacklist' tags may be dropped but all others will never be, 'None' may drop any tags. Can specify either a delimiter-separated list in the field, or a file path to a .txt or .csv file with entries separated by newlines.",
  "concept.text.tag_dropout_special_tags_mode": "Select whether special tags act as a whitelist, blacklist, or are disabled.",
  "concept.text.tag_dropout_special_tags_regex": "Interpret special tags with regex, such as 'photo.*' to match 'photo, photograph, photon' but not 'telephoto'. Includes exception for '/(' and '/)' syntax found in many booru/e6 tags.",
  "concept.text_variations": "The number of different text versions to cache if latent caching is enabled.",
  "concept.type": "STANDARD: Standard finetuning with the sample as training target\nVALIDATION: Use concept for validation instead of training\nPRIOR_PREDICTION: Use the sample to make a prediction using the model as it was before training. This prediction is then used as the training target for the model in training. This can be used as regularisation and to preserve prior model knowledge while finetuning the model on other concepts. Only implemented for LoRA.",
  "concept_file_name": "Concept file name",
  "concept_stats": "Concept stats",
  "concepts": "Concepts",
  "continue_last_backup": "Automatically continues training from the last backup saved in <workspace>/backup",
  "create": "Create",
  "custom_conditioning_image": "When custom conditioning image is enabled, will use png postfix with -condlabel instead of automatically generated. It's suitable for special scenarios, such as object removal, allowing the model to learn a certain behavior concept",
  "custom_learning_rate_scheduler": "Custom learning rate scheduler",
  "d0": "D0",
  "d_coef": "D coef",
  "d_limiter": "D limiter",
  "dampening": "Dampening",
  "dataloader_threads": "Number of threads used for the data loader. Increase if your GPU has room during caching, decrease if it's going out of memory during caching.",
  "debug_dir": "The directory where debug data is saved",
  "debug_mode": "Save debug information during the training into the debug directory",
  "decay_rate": "Decay rate",
  "decoder": "Decoder",
  "decoder.model_name": "Filename, directory or Hugging Face repository of the decoder model",
  "decoder.weight_dtype": "The decoder weight data type",
  "decoder_text_encoder": "Decoder text encoder",
  "decoder_text_encoder.weight_dtype": "The decoder text encoder weight data type",
  "decoder_vqgan": "Decoder vqgan",
  "decoder_vqgan.weight_dtype": "The decoder vqgan weight data type",
  "decouple": "Decouple",
  "decoupled_decay": "Decoupled decay",
  "degenerated_to_sgd": "Degenerated to sgd",
  "delete_workspace": "Delete workspace",
  "detach_trainer": "Detach trainer",
  "device_indexes": "Multi-GPU: A comma-separated list of device indexes. If empty, all your GPUs are used. With a list such as \"0,1,3,4\" you can omit a GPU, for example an on-board graphics GPU.",
  "differentiable": "Differentiable",
  "diffusion_steps": "Diffusion steps",
  "download_backups": "Download backups",
  "download_output_model": "Download output model",
  "download_samples": "Download samples",
  "download_saves": "Download saves",
  "download_tensorboard": "Download tensorboard",
  "dropout_probability": "Dropout probability. This percentage of model nodes will be randomly ignored at each training step. Helps with overfitting. 0 disables, 1 maximum.",
  "dynamic_timestep_shifting": "Dynamically shift the timestep distribution based on resolution. If enabled, the shifting parameters are taken from the model's scheduler configuration and Timestep Shift is ignored. Note: For Z-Image and Flux2, the dynamic shifting parameters are likely wrong and unknown. Use with care or set your own, fixed shift.",
  "effnet_encoder": "Effnet encoder",
  "effnet_encoder.model_name": "Filename, directory or Hugging Face repository of the effnet encoder model",
  "effnet_encoder.weight_dtype": "The effnet encoder weight data type",
  "ema": "EMA averages the training progress over many steps, better preserving different concepts in big datasets",
  "ema_decay": "Decay parameter of the EMA model. Higher numbers will average more steps. For datasets of hundreds or thousands of images, set this to 0.9999. For smaller datasets, set it to 0.999 or even 0.998",
  "ema_update_step_interval": "Number of steps between EMA update steps",
  "embedding": "Embedding",
  "embedding.initial_embedding_text": "The initial embedding text used when creating a new embedding",
  "embedding.is_output_embedding": "Output embeddings are calculated at the output of the text encoder, not the input. This can improve results for larger text encoders and lower VRAM usage.",
  "embedding.model_name": "The base embedding to train on. Leave empty to create a new embedding",
  "embedding.placeholder": "The placeholder used when using the embedding in a prompt",
  "embedding.stop_training_after": "When to stop training the embedding",
  "embedding.token_count": "The token count used when creating a new embedding. Leave empty to auto detect from the initial embedding text.",
  "embedding.train": "Enable or disable training of the embedding",
  "embedding_learning_rate": "The learning rate of embeddings. Overrides the base learning rate",
  "embedding_weight_dtype": "The Embedding weight data type used for training. This can reduce memory consumption, but reduces precision",
  "enable_activation_offloading": "Offload activations to CPU during the forward pass to reduce GPU memory usage. May slow training slightly due to CPU-GPU transfers",
  "enable_async_offloading": "Enable asynchronous layer offloading to overlap GPU compute with CPU-GPU transfers. Can improve throughput when using layer offloading",
  "enable_autocast_cache": "Enables the autocast cache. Disabling this reduces memory usage, but increases training time",
  "enable_crop_jitter": "Enable crop jitter",
  "enable_fixed_brightness": "Enable fixed brightness",
  "enable_fixed_contrast": "Enable fixed contrast",
  "enable_fixed_flip": "Enable fixed flip",
  "enable_fixed_hue": "Enable fixed hue",
  "enable_fixed_rotate": "Enable fixed rotate",
  "enable_fixed_saturation": "Enable fixed saturation",
  "enable_random_brightness": "Enable random brightness",
  "enable_random_circular_mask_shrink": "Enable random circular mask shrink",
  "enable_random_contrast": "Enable random contrast",
  "enable_random_flip": "Enable random flip",
  "enable_random_hue": "Enable random hue",
  "enable_random_mask_rotate_crop": "Enable random mask rotate crop",
  "enable_random_rotate": "Enable random rotate",
  "enable_random_saturation": "Enable random saturation",
  "enable_resolution_override": "Enable resolution override",
  "enable_tag_shuffling": "Enable tag shuffling",
  "enabled": "Enabled",
  "epochs": "The number of epochs for a full training run",
  "eps": "Eps",
  "eps2": "Eps2",
  "factored": "Factored",
  "factored_fp32": "Factored fp32",
  "fallback_train_dtype": "The mixed precision data type used for training stages that don't support float16 data types. This can increase training speed, but reduces precision",
  "file_sync": "File sync",
  "fixed_decay": "Fixed decay",
  "force_circular_padding": "Enables circular padding for all conv layers to better train seamless images",
  "force_epsilon_prediction": "Force epsilon prediction",
  "force_last_timestep": "Force last timestep",
  "force_v_prediction": "Force v prediction",
  "foreach": "Foreach",
  "frames": "The number of frames used for training.",
  "fsdp_in_use": "Fsdp in use",
  "fused": "Fused",
  "fused_back_pass": "Fused back pass",
  "fused_gradient_reduce": "Multi-GPU: Gradient synchronisation during the backward pass. Can be more efficient, especially with Async Gradient Reduce",
  "generalized_offset_noise": "Per-timestep 'brightness knob' instead of a fixed offset - steadier training, better starts, and improved very dark/bright images. Compatible with V-pred and Eps-pred. Start with 0.02 and adjust as needed.",
  "gpu_type": "Gpu type",
  "gradient_accumulation_steps": "Number of accumulation steps. Increase this number to trade batch size for training speed",
  "gradient_checkpointing": "Enables gradient checkpointing. This reduces memory usage, but increases training time",
  "gradient_reduce_precision": "WEIGHT_DTYPE: Reduce gradients between GPUs in your weight data type; can be imprecise, but more efficient than float32\nWEIGHT_DTYPE_STOCHASTIC: Sum up the gradients in your weight data type, but average them in float32 and stochastically round if your weight data type is bfloat16\nFLOAT_32: Reduce gradients in float32\nFLOAT_32_STOCHASTIC: Reduce gradients in float32; use stochastic rounding to bfloat16 if your weight data type is bfloat16",
  "grams_moment": "Grams moment",
  "growth_rate": "Growth rate",
  "guidance_scale": "Guidance scale",
  "height": "Height",
  "host": "Host",
  "huber_delta": "Delta parameter for huber loss",
  "huber_strength": "Huber loss strength for custom loss settings. Less sensitive to outliers than MSE. Strengths should generally sum to 1.",
  "huggingface_cache_dir": "Huggingface cache dir",
  "huggingface_token": "Huggingface token",
  "id": "Id",
  "image": "Image",
  "image_variations": "Image variations",
  "include": "Include",
  "include_subdirectories": "Include subdirectories",
  "include_train_config": "Include the training configuration in the final model. Only supported for safetensors files. None: No config is included. Settings: All training settings are included. All: All settings, including the samples and concepts are included.",
  "initial_accumulator": "Initial accumulator",
  "initial_accumulator_value": "Initial accumulator value",
  "initial_embedding_text": "Initial embedding text",
  "install_cmd": "Install cmd",
  "install_onetrainer": "Install onetrainer",
  "is_output_embedding": "Is output embedding",
  "is_paged": "Is paged",
  "k": "K",
  "k_warmup_steps": "K warmup steps",
  "kappa_p": "Kappa p",
  "keep_tags_count": "Keep tags count",
  "key_file": "Key file",
  "kourkoutas_beta": "Kourkoutas beta",
  "latent_caching": "Caching of intermediate training data that can be re-used between epochs",
  "layer_filter": "Comma-separated list of diffusion layers to train. Regular expressions (if toggled) are supported. Any model layer with a matching name will be trained",
  "layer_filter_preset": "Select a preset defining which layers to train, or select 'Custom' to define your own.\nA blank 'custom' field or 'Full' will train all layers.",
  "layer_filter_regex": "If enabled, layer filter patterns are interpreted as regular expressions. Otherwise, simple substring matching is used.",
  "layer_offload_fraction": "Enables offloading of individual layers during training to reduce VRAM usage. Increases training time and uses more RAM. Only available if checkpointing is set to CPU_OFFLOADED. values between 0 and 1, 0=disabled",
  "learning_rate": "The base learning rate",
  "learning_rate_cycles": "The number of learning rate cycles. This is only applicable if the learning rate scheduler supports cycles",
  "learning_rate_min_factor": "Unit = float. Method = percentage. For a factor of 0.1, the final LR will be 10% of the initial LR. If the initial LR is 1e-4, the final LR will be 1e-5.",
  "learning_rate_scaler": "Selects the type of learning rate scaling to use during training. Functionally equated as: LR * SQRT(selection)",
  "learning_rate_scheduler": "Learning rate scheduler that automatically changes the learning rate during training",
  "learning_rate_warmup_steps": "The number of steps it takes to gradually increase the learning rate from 0 to the specified learning rate. Values >1 are interpreted as a fixed number of steps, values <=1 are interpreted as a percentage of the total training steps (ex. 0.2 = 20% of the total step count)",
  "length": "Length",
  "log_cosh_strength": "Log - Hyperbolic cosine Error strength for custom loss settings. Strengths should generally sum to 1.",
  "log_every": "Log every",
  "lora_alpha": "The alpha parameter used when creating a new LoRA",
  "lora_decompose": "Decompose LoRA Weights (aka, DoRA).",
  "lora_decompose_norm_epsilon": "Add an epsilon to the norm division calculation in DoRA. Can aid in training stability, and also acts as regularization.",
  "lora_decompose_output_axis": "Apply the weight decomposition on the output axis instead of the input axis.",
  "lora_model_name": "The base LoRA to train on. Leave empty to create a new LoRA",
  "lora_rank": "The rank parameter used when creating a new LoRA",
  "lora_weight_dtype": "The LoRA weight data type used for training. This can reduce memory consumption, but reduces precision",
  "loss_scaler": "Selects the type of loss scaling to use during training. Functionally equated as: Loss * selection",
  "loss_weight": "Loss weight",
  "loss_weight_fn": "Choice of loss weight function. Can help the model learn details more accurately.",
  "loss_weight_strength": "Inverse strength of loss weighting. Range: 1-20, only applies to Min SNR and P2.",
  "low_rank_ortho": "Low rank ortho",
  "lr_decay": "Lr decay",
  "mae_strength": "Mean Absolute Error strength for custom loss settings. Strengths should generally sum to 1.",
  "mask_image_path": "Mask image path",
  "masked_prior_preservation_weight": "Preserves regions outside the mask using the original untrained model output as a target. Only available for LoRA training. If enabled, use a low unmasked weight.",
  "masked_training": "Masks the training samples to let the model focus on certain parts of the image. When enabled, one mask image is loaded for each training sample.",
  "max_noising_strength": "Specifies the maximum noising strength used during training. This can be useful to reduce overfitting, but also reduces the impact of training samples on the overall image composition",
  "max_unorm": "Max unorm",
  "maximize": "Maximize",
  "min_8bit_size": "Min 8bit size",
  "min_beta1": "Min beta1",
  "min_download": "Min download",
  "min_noising_strength": "Specifies the minimum noising strength used during training. This can help to improve composition, but prevents finer details from being trained",
  "model_name": "Model name",
  "model_type": "Model type",
  "momentum": "Momentum",
  "mse_strength": "Mean Squared Error strength for custom loss settings. Strengths should generally sum to 1.",
  "multi_gpu": "Enable multi-GPU training",
  "muon_adam_config": "Muon adam config",
  "muon_adam_lr": "Muon adam lr",
  "muon_adam_regex": "Muon adam regex",
  "muon_hidden_layers": "Muon hidden layers",
  "muon_te1_adam_lr": "Muon te1 adam lr",
  "muon_te2_adam_lr": "Muon te2 adam lr",
  "n_sma_threshold": "N sma threshold",
  "name": "Name",
  "negative_prompt": "Negative prompt",
  "nesterov": "Nesterov",
  "nnmf_factor": "Nnmf factor",
  "no_prox": "No prox",
  "noise_scheduler": "Noise scheduler",
  "noising_bias": "Controls the bias parameter of the timestep distribution function. Use the preview to see more details.",
  "noising_weight": "Controls the weight parameter of the timestep distribution function. Use the preview to see more details.",
  "non_ema_sampling": "Whether to include non-ema sampling when using ema.",
  "normalize_masked_area_loss": "When masked training is enabled, normalizes the loss for each sample based on the sizes of the masked region",
  "normuon_eps": "Normuon eps",
  "normuon_variant": "Normuon variant",
  "ns_steps": "Ns steps",
  "offset_noise_weight": "The weight of offset noise added to each training step",
  "oft_block_share": "Share the OFT parameters between blocks. A single rotation matrix is shared across all blocks within a layer, drastically cutting the number of trainable parameters and yielding very compact adapter files.",
  "oft_block_size": "The block size parameter used when creating a new OFT v2",
  "oft_coft": "Use the constrained variant of OFT. This constrains the learned rotation to stay very close to the identity matrix, limiting adaptation to only small changes. This improves training stability, helps prevent overfitting on small datasets, and better preserves the base model's original knowledge but it may lack expressiveness for tasks requiring substantial adaptation.",
  "on_detached_error": "On detached error",
  "on_detached_finish": "On detached finish",
  "on_error": "On error",
  "on_finish": "On finish",
  "onetrainer_dir": "Onetrainer dir",
  "only_cache": "Only populate the cache, without any training",
  "optim_bits": "Optim bits",
  "optimizer": "Optimizer",
  "optimizer.optimizer": "The type of optimizer",
  "optimizer_defaults": "Optimizer defaults",
  "ortho_rank": "Ortho rank",
  "orthogonal_gradient": "Orthogonal gradient",
  "output_dtype": "Precision to use when saving the output model",
  "output_model_destination": "Filename or directory where the output model is saved",
  "output_model_format": "Format to use when saving the output model",
  "password": "Password",
  "path": "Path",
  "peft_type": "The type of low-parameter finetuning method.",
  "percentile_clipping": "Percentile clipping",
  "perturbation_noise_weight": "The weight of perturbation noise added to each training step",
  "placeholder": "Placeholder",
  "port": "Port",
  "preserve_embedding_norm": "Rescales each trained embedding to the median embedding norm",
  "prevent_overwrites": "Prevent overwrites",
  "prior": "Prior",
  "prior.learning_rate": "The learning rate of the Prior. Overrides the base learning rate",
  "prior.model_name": "Filename, directory or Hugging Face repository of the prior model",
  "prior.stop_training_after": "When to stop training the Prior",
  "prior.train": "Enables training the Prior model",
  "prior.weight_dtype": "The prior weight data type",
  "prodigy_steps": "Prodigy steps",
  "prompt": "Prompt",
  "prompt_path": "Prompt path",
  "prompt_source": "Prompt source",
  "quant_block_size": "Quant block size",
  "quantization": "Quantization",
  "quantization.layer_filter": "Comma-separated list of layers to quantize. Regular expressions (if toggled) are supported. Any model layer with a matching name will be quantized",
  "quantization.layer_filter_preset": "Select a preset defining which layers to quantize. Quantization of certain layers can decrease model quality. Only applies to the transformer/unet",
  "quantization.layer_filter_regex": "If enabled, layer filter patterns are interpreted as regular expressions. Otherwise, simple substring matching is used.",
  "quantization.svd_dtype": "What datatype to use for SVDQuant weights decomposition.",
  "quantization.svd_rank": "Rank for SVDQuant weights decomposition",
  "r": "R",
  "random_brightness_max_strength": "Random brightness max strength",
  "random_contrast_max_strength": "Random contrast max strength",
  "random_hue_max_strength": "Random hue max strength",
  "random_rotate_max_angle": "Random rotate max angle",
  "random_saturation_max_strength": "Random saturation max strength",
  "random_seed": "Random seed",
  "rectify": "Rectify",
  "relative_step": "Relative step",
  "remote_dir": "Remote dir",
  "rescale_noise_scheduler_to_zero_terminal_snr": "Rescales the noise scheduler to a zero terminal signal to noise ratio and switches the model to a v-prediction target",
  "resolution": "The resolution used for training. Optionally specify multiple resolutions separated by a comma, or a single exact resolution in the format <width>x<height>",
  "resolution_override": "Resolution override",
  "rms_rescaling": "Rms rescaling",
  "rolling_backup": "If rolling backups are enabled, older backups are deleted automatically",
  "rolling_backup_count": "Defines the number of backups to keep if rolling backups are enabled",
  "run_id": "Run id",
  "safeguard_warmup": "Safeguard warmup",
  "sample.base_image_path": "The base image used when inpainting.",
  "sample.frames": "Number of frames to generate. Only used when generating videos.",
  "sample.length": "Length in seconds of audio output.",
  "sample.mask_image_path": "The mask used when inpainting.",
  "sample.sample_inpainting": "Enables inpainting sampling. Only available when sampling from an inpainting model.",
  "sample_after": "The interval used when automatically sampling from the model during training",
  "sample_after_unit": "Sample after unit",
  "sample_audio_format": "Sample audio format",
  "sample_definition_file_name": "Sample definition file name",
  "sample_image_format": "File Format used when saving samples",
  "sample_inpainting": "Sample inpainting",
  "sample_skip_first": "Start sampling automatically after this interval has elapsed.",
  "sample_video_format": "Sample video format",
  "samples": "Samples",
  "samples_to_tensorboard": "Whether to include sample images in the Tensorboard output.",
  "save_every": "The interval used when automatically saving the model during training",
  "save_every_unit": "Save every unit",
  "save_filename_prefix": "The prefix for filenames used when saving the model during training",
  "save_skip_first": "Start saving automatically after this interval has elapsed",
  "scale_parameter": "Scale parameter",
  "schedulefree_c": "Schedulefree c",
  "scheduler_params": "Scheduler params",
  "secrets": "Secrets",
  "secrets.cloud.api_key": "Cloud service API key for RUNPOD. Leave empty for LINUX. This value is stored separately, not saved to your configuration file.",
  "secrets.cloud.host": "SSH server hostname or IP. Leave empty if you have a Cloud ID or want to automatically create a new cloud.",
  "secrets.cloud.id": "RUNPOD Cloud ID. The cloud service must have a public IP and SSH service. Leave empty if you want to automatically create a new RUNPOD cloud, or if you're connecting to another cloud provider via SSH Hostname and Port.",
  "secrets.cloud.key_file": "Absolute path to the private key file used for SSH connections. Leave empty to rely on your system SSH configuration.",
  "secrets.cloud.password": "SSH password for password-based authentication. If you try to use native SCP requires sshpass to be installed. Leave empty to use key-based authentication.",
  "secrets.cloud.port": "SSH server port. Leave empty if you have a Cloud ID or want to automatically create a new cloud.",
  "secrets.cloud.user": "SSH username. Use \"root\" for RUNPOD. Your SSH client must be set up to connect to the cloud using a public key, without a password. For RUNPOD, create an ed25519 key locally, and copy the contents of the public keyfile to your \"SSH Public Keys\" on the RunPod website.",
  "secrets.huggingface_token": "Enter your Hugging Face access token if you have used a protected Hugging Face repository below.\nThis value is stored separately, not saved to your configuration file. Go to https://huggingface.co/settings/tokens to create an access token.",
  "seed": "Seed",
  "slice_p": "Slice p",
  "split_groups": "Split groups",
  "split_groups_mean": "Split groups mean",
  "stochastic_rounding": "Stochastic rounding",
  "stop_training_after": "Stop training after",
  "stop_training_after_unit": "Stop training after unit",
  "sub_type": "Sub type",
  "svd_dtype": "Svd dtype",
  "svd_rank": "Svd rank",
  "tag_delimiter": "Tag delimiter",
  "tag_dropout_enable": "Tag dropout enable",
  "tag_dropout_mode": "Tag dropout mode",
  "tag_dropout_probability": "Tag dropout probability",
  "tag_dropout_special_tags": "Tag dropout special tags",
  "tag_dropout_special_tags_mode": "Tag dropout special tags mode",
  "tag_dropout_special_tags_regex": "Tag dropout special tags regex",
  "temp_device": "The device used to temporarily offload models while they are not used. Default:\"cpu\"",
  "tensorboard": "Starts the Tensorboard Web UI during training",
  "tensorboard_always_on": "Keep Tensorboard accessible even when not training. Useful for monitoring completed training sessions.",
  "tensorboard_expose": "Exposes Tensorboard Web UI to all network interfaces (makes it accessible from the network)",
  "tensorboard_port": "Port to use for Tensorboard link",
  "tensorboard_tunnel": "Tensorboard tunnel",
  "text": "Text",
  "text_encoder": "Text encoder",
  "text_encoder.dropout_probability": "The Probability for dropping the text encoder conditioning",
  "text_encoder.include": "Includes the text encoder in the training run",
  "text_encoder.learning_rate": "The learning rate of the text encoder. Overrides the base learning rate",
  "text_encoder.stop_training_after": "When to stop training the text encoder",
  "text_encoder.train": "Enables training the text encoder model",
  "text_encoder.train_embedding": "Enables training embeddings for the text encoder model",
  "text_encoder.weight_dtype": "The text encoder weight data type",
  "text_encoder_1_layer_skip": "Text encoder 1 layer skip",
  "text_encoder_1_sequence_length": "Text encoder 1 sequence length",
  "text_encoder_2": "Text encoder 2",
  "text_encoder_2.dropout_probability": "The Probability for dropping the text encoder 2 conditioning",
  "text_encoder_2.include": "Includes text encoder 2 in the training run",
  "text_encoder_2.learning_rate": "The learning rate of the text encoder 2. Overrides the base learning rate",
  "text_encoder_2.stop_training_after": "When to stop training the text encoder 2",
  "text_encoder_2.train": "Enables training the text encoder 2 model",
  "text_encoder_2.train_embedding": "Enables training embeddings for the text encoder 2 model",
  "text_encoder_2.weight_dtype": "The text encoder 2 weight data type",
  "text_encoder_2_layer_skip": "The number of additional clip layers to skip. 0 = the model default",
  "text_encoder_2_sequence_length": "Overrides the number of tokens used for captions. If empty, the model default is used, which is 512 on Flux. Comfy samples with 256 tokens though. 77 is the default only for backwards compatibility.",
  "text_encoder_3": "Text encoder 3",
  "text_encoder_3.dropout_probability": "The Probability for dropping the text encoder 3 conditioning",
  "text_encoder_3.include": "Includes text encoder 3 in the training run",
  "text_encoder_3.learning_rate": "The learning rate of the text encoder 3. Overrides the base learning rate",
  "text_encoder_3.stop_training_after": "When to stop training the text encoder 3",
  "text_encoder_3.train": "Enables training the text encoder 3 model",
  "text_encoder_3.train_embedding": "Enables training embeddings for the text encoder 3 model",
  "text_encoder_3.weight_dtype": "The text encoder 3 weight data type",
  "text_encoder_3_layer_skip": "The number of additional clip layers to skip. 0 = the model default",
  "text_encoder_4": "Text encoder 4",
  "text_encoder_4.dropout_probability": "The Probability for dropping the text encoder 4 conditioning",
  "text_encoder_4.include": "Includes text encoder 4 in the training run",
  "text_encoder_4.learning_rate": "The learning rate of the text encoder 4. Overrides the base learning rate",
  "text_encoder_4.model_name": "Filename, directory or Hugging Face repository of the text encoder 4 model",
  "text_encoder_4.stop_training_after": "When to stop training the text encoder 4",
  "text_encoder_4.train": "Enables training the text encoder 4 model",
  "text_encoder_4.train_embedding": "Enables training embeddings for the text encoder 4 model",
  "text_encoder_4.weight_dtype": "The text encoder 4 weight data type",
  "text_encoder_4_layer_skip": "Text encoder 4 layer skip",
  "text_encoder_layer_skip": "The number of additional clip layers to skip. 0 = the model default",
  "text_encoder_sequence_length": "Number of tokens for captions",
  "text_variations": "Text variations",
  "timestep_distribution": "Selects the function to sample timesteps during training",
  "timestep_shift": "Shift the timestep distribution. Use the preview to see more details.",
  "token_count": "Token count",
  "tools.convert_model": "Open the model conversion tool",
  "tools.dataset": "Open the captioning tool",
  "tools.profiling": "Open the profiling tools.",
  "tools.sampling": "Open the model sampling tool",
  "tools.video": "Open the video tools",
  "train": "Train",
  "train_device": "The device used for training. Can be \"cuda\", \"cuda:0\", \"cuda:1\" etc. Default:\"cuda\". Must be \"cuda\" for multi-GPU training.",
  "train_dtype": "The mixed precision data type used for training. This can increase training speed, but reduces precision",
  "train_embedding": "Train embedding",
  "training_method": "Training method",
  "transformer": "Transformer",
  "transformer.attention_mask": "Force enables passing of a text embedding attention mask to the transformer. This can improve training on shorter captions.",
  "transformer.guidance_scale": "The guidance scale of guidance distilled models passed to the transformer during training.",
  "transformer.learning_rate": "The learning rate of the Transformer. Overrides the base learning rate",
  "transformer.model_name": "Can be used to override the transformer in the base model. Safetensors and GGUF files are supported, local and on Huggingface. If a GGUF file is used, the DataType must also be set to GGUF",
  "transformer.stop_training_after": "When to stop training the Transformer",
  "transformer.train": "Enables training the Transformer model",
  "transformer.weight_dtype": "The transformer weight data type",
  "transformer_attention_mask": "Transformer attention mask",
  "type": "Type",
  "unet": "Unet",
  "unet.learning_rate": "The learning rate of the UNet. Overrides the base learning rate",
  "unet.stop_training_after": "When to stop training the UNet",
  "unet.train": "Enables training the UNet model",
  "unet.weight_dtype": "The unet weight data type",
  "unmasked_probability": "When masked training is enabled, specifies the number of training steps done on unmasked samples",
  "unmasked_weight": "When masked training is enabled, specifies the loss weight of areas outside the masked region",
  "update_onetrainer": "Update onetrainer",
  "use_AdEMAMix": "Use AdEMAMix",
  "use_adopt": "Use adopt",
  "use_atan2": "Use atan2",
  "use_bias_correction": "Use bias correction",
  "use_cautious": "Use cautious",
  "use_grams": "Use grams",
  "use_orthograd": "Use orthograd",
  "use_schedulefree": "Use schedulefree",
  "use_speed": "Use speed",
  "use_stableadamw": "Use stableadamw",
  "use_triton": "Use triton",
  "user": "User",
  "uuid": "Uuid",
  "vae": "Vae",
  "vae.model_name": "Directory or Hugging Face repository of a VAE model in diffusers format. Can be used to override the VAE included in the base model. Using a safetensor VAE file will cause an error that the model cannot be loaded.",
  "vae.weight_dtype": "The vae weight data type",
  "validate_after": "The interval used when validate training",
  "validate_after_unit": "Validate after unit",
  "validation": "Enable validation steps and add new graph in tensorboard",
  "vb_loss_strength": "Variational lower-bound strength for custom loss settings. Should be set to 1 for variational diffusion models",
  "volume_size": "Volume size",
  "warmup_init": "Warmup init",
  "weight_decay": "Weight decay",
  "weight_decay_by_lr": "Weight decay by lr",
  "weight_dtype": "Weight dtype",
  "weight_lr_power": "Weight lr power",
  "width": "Width",
  "workspace_dir": "The directory where all files of this training run are saved",
  "xi": "Xi",
};

/** Field keys that require wide tooltip display. */
export const WIDE_TOOLTIP_KEYS: Set<string> = new Set([
  "dynamic_timestep_shifting",
  "gradient_reduce_precision",
  "secrets.huggingface_token",
  "timestep_distribution",
]);

/** Get tooltip text for a config field key. Returns undefined if not found. */
export function getTooltip(fieldKey: string): string | undefined {
  return FIELD_TOOLTIPS[fieldKey];
}
